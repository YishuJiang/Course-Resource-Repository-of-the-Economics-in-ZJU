\chapter{动态随机一般均衡基础}
本章介绍DSGE的一些基础知识, 包括集值映射、Bellman方程、最优化和动态规划等, 并简单介绍一下引入随机性(风险)的情形.主要参考书为《动态经济的递归方法》.
同时为了写公式方便, 笔者这里在涉及到较多公式的区域时会直接写英语(\sout{也是在复刻邬sir的上课风格, 绝对不是因为作者比较懒什么的}).

{\kaishu 这一章的主线剧情是求解贝尔曼方程以分析离散时间的确定性的无穷期的问题, 最初的想法是将序列问题转化成递归问题, 于是就需要搞定两件事:1. 序列问题是否和递归问题等价(比如说, 万一我们得到的递归问题中的值函数是随时间变动的, 那很有可能就会出现序列问题与递归问题不等价或者问题处理起来会变得更加复杂); 
2. 在等价的情形下, Bellman方程的解是否存在、如果存在应当如何求解、求出结果后这个结果又是否是唯一的.要搞定第一个问题, 需要引入对“对应”的刻画以及最大值定理; 而对于第二个问题, 需要我们对Bellman算子进行分析, 从而需要引入度量空间和压缩映射等概念.
完成了这些工作时候, 又可以再进一步扩展, 看看引入随机性后的模型又当如何处理, 当然这一部分以目前的知识积累还无法扩展太多.

最后的最后, 虽然这一章的内容学起来会觉得很复杂很高大上, 但这些只是冰山一角, 基于这些知识所能求解的模型也是很有限很简单的. 有点令人遗憾, 但这这正是宏观经济学的魅力所在.}

\section{Bellman方程}
在之前的无穷期模型中, 我们已经接触到了Bellman方程, 通过迭代的方法求解无穷期的最优化问题, 从而有了Bellman方程.
但Bellman方程的求解是相当困难的, 并且对于这样一个形式的方程, 我们在求解之前首先得讨论它的解的存在性和唯一性.
\subsection{Bellman方程的基本形式}
考虑如下最优化问题：
$$w(k_0)=\max_{\{c_t,k_t,n_t\}_{t=0}^{\infty}}\sum_{n=0}^{\infty}$$
\subsection{度量空间与压缩映射}
{\kaishu 通过之前的讨论, Bellman方程的解定义在了函数空间上, 现在的目标是将$\mathbb{R}^n$上的性质相似地迁移到函数空间上.
为此, 我们需要引入度量空间、收敛、完备性、压缩映射等概念.}
\subsubsection{度量空间}
首先我们给出度量的概念, 它用于刻画“距离”的概念.
\begin{definition}{metric and metric space}
    A Metric Space is a set $S$ and a function $d: S \times S \to \mathbb{R}$ such that for all $x, y, z \in S$:
    \begin{enumerate}
        \item $d(x, y) \geq 0$ (non-negativity) and $d(x, y) = 0 \iff x = y$ (identity of indiscernibles)
        \item $d(x, y) = d(y, x)$ (symmetry)
        \item $d(x, y) + d(y, z) \geq d(x, z)$ (triangle inequality)
    \end{enumerate}
    The function $d$ is called a metric.
\end{definition}

\begin{definition}{covegence}
    A sequence $\{x_n\}$ in a metric space $(S, d)$ is said to converge to $x \in S$ if for every $\varepsilon > 0$, there exists an $N$ such that $d(x_n, x) < \varepsilon$ for all $n \geq N$.
\end{definition}

\begin{definition}{completeness}
    A metric space $(S, d)$ is said to be complete if every Cauchy sequence in $S$ converges to a point in $S$. 
    A sequence $\{x_n\}$ is said to be Cauchy if for every $\varepsilon > 0$, there exists an $N$ such that $d(x_n, x_m) \leq  \varepsilon$ for all $n, m \geq N$.
\end{definition}
\newpage

\subsubsection{Banach空间与压缩映射定理}
接着我们要更进一步, 刻画单个元素的“大小”. 为此我们引入范数的概念.
\begin{definition}{norm and norm space}
    A norm on a vector space $V$ is a function $\|\cdot\|: V \to \mathbb{R}$ such that for all $x, y \in V$ and $\alpha \in \mathbb{R}$:
    \begin{enumerate}
        \item $\|x\| \geq 0$ (non-negativity) and $\|x\| = 0 \iff x = 0$ (identity of indiscernibles)
        \item $\|\alpha x\| = |\alpha| \|x\|$ (positive-homogeneity)
        \item $\|x + y\| \leq \|x\| + \|y\|$ (triangle inequality)
    \end{enumerate}
\end{definition}
以下是一些函数空间上一些常见的范数.
\begin{itemize}
    \item $L^p$范数:
        $$\|f\|_p = \left(\int |f|^p dx\right)^{1/p}$$
    \item $L^\infty$范数(又称无穷范数): 
        $$\|f\|_\infty = \sup |f|$$
    \item $L^2$范数(也就是常说的欧式范数): 
        $$\|f\|_2 = \left(\int |f|^2 {\rm d}x\right)^{1/2}$$
    \item $L^1$范数(俗称曼哈顿范数): $L^1$范数是定义在函数空间上的范数, 它的定义如下:
        $$\|f\|_1 = \int |f| {\rm d}x$$
\end{itemize}
事实上, 我们可以用范数定义度量, 换言之, 赋范线性空间一定是度量空间.在赋范线性空间的基础上, 再加上完备性, 我们就得到了Banach空间.
\begin{definition}{Banach space}
    A Banach Space is a normed vector space that is complete with respect to the metric induced by the norm.
\end{definition}
Banach空间的性质相当优良, 对于压缩映射具有不动点定理, 可以用来证明存在唯一性
(例如数学分析中的隐函数定理、ODE中的Picard定理, 都可以通过压缩映射定理加以证明).
\begin{definition}{contraction mapping}
    Let $(X,d)$ be a metric space.A mapping $T: X \to X$ is said to be a contraction if there exists a constant $0 < \beta < 1$ such that for all $x, y \in X$:
    $$d(T(x),T(y)) \leq \beta d(x,y)$$
    We call $\beta$ the module of $T$.
\end{definition}
压缩映射是一类特殊的映射, 随着迭代的进行 ,两个元素的相之间的“距离”会越来越小. 事实上, 可以看出它具有一致连续性.
\begin{lemma}
    Let $(S,d)$ be a metric space and $T:S\to S$ be a contraction mapping, then $T$ is uniformly continous. 
\end{lemma}
\begin{proof}
    For any $\varepsilon>0$, let $\delta = \frac{\varepsilon}{\beta}$. 
    Since $d(Tx,Ty)\leq \beta d(x,y)$, if $d(x,y)\leq \delta$, then $d(Tx,Ty)<\varepsilon$. 
\end{proof}
\newpage

接下来给出本章最重要的一个定理, 正如书中所说的, 简洁且强大.
注意这里只要是完备的度量空间即可, 因为只涉及到了"距离"的概念, 不用像书中强到Banach空间.
\begin{theorem}{Banach fixed point theorem}
    Let $(S,d)$ be a complete metric space and suppose $T:S\to S$ is a contraction mapping with module $\beta$. Then:
    \begin{itemize}
        \item the operator $T$ has excatly one fixed point $v^*\in S$.
        \item For any $v_0\in S$ and for any $n\in\mathbb{N}$, $d(T^nv_0,v^*)\leq \beta^nd(v_0,v^*)$, that means for any $v_0\in S$ ,$\{T^nv_0\}_{n=0}^{\infty}$ converges to $v^*$.
    \end{itemize}
\end{theorem}
\begin{proof}
    \begin{itemize}
        \item For all $v_0\in S$, let $v_1=Tv_0,v_2=Tv_1=T^2v_0,\cdots,v_n=Tv_{n-1}=T^nv_0,\cdots$.\\
        Since $T$ is a contraction mapping, then 
        $$d(v_{n+1},v_n)=d(Tv_n,Tv_{n-1})\leq \beta d(v_n,v_{n-1})\leq \cdots \leq \beta^n d(v_1,v_0)$$
        By triangle inequality, for $m>n$, we have
        $$
        \begin{aligned}
            d(v_m,v_n) & \leq d(v_m,v_{m-1}) + d(v_{m-1},v_{m-2}) + \cdots + d(v_{n+1},v_n) \\
            & \leq \beta^{m}d(v_1,v_0) + \beta^{m-1}d(v_1,v_0) + \cdots + \beta^n d(v_1,v_0) \\
            & \leq \frac{\beta^n}{1-\beta}d(v_1,v_0)
            \end{aligned}$$
        By chosing large enough $n$, $d(v_m,v_n)$ can be small enough, so $\{v_n\}_{n=0}^{\infty}$ is a Cauchy sequence.\\
        Since $(S,d)$ is a complete metric space, $\{v_n\}_{n=0}^{\infty}$ converges to a limit $v^*\in S$.\\
        $Tv^*=T(\lim_{n\to\infty}v_n)$. By continuity of $T$, $T(\lim_{n\to\infty}v_n)=\lim_{n\to\infty}Tv_n$.
        Since $lim_{n\to\infty}Tv_n=\lim_{n\to\infty}v_{n+1}=v^*$, thus we got $Tv^*=v^*$.\\
        Suppose there exists another fixed point $\bar{v}\neq v^*$ such $T\bar{v}=\bar{v}$.
        Then $d(\bar{v},v^*)=d(T\bar{v},Tv^*)\leq \beta d(\bar{v},v^*)$, that is impossible.
        \item Use mathematic indunction to prove.\\
        For $n=0$, $d(v_0,v^*)\leq \beta^0 d(v_0,v^*)$.\\
        Suppose for $n=k$, $d(v_k,v^*)\leq \beta^k d(v_0,v^*)$.\\
        For $n=k+1$, $d(v_{k+1},v^*)=d(Tv_k,v^*)\leq \beta d(v_k,v^*)\leq \beta^{k+1}d(v_0,v^*)$.\\
    \end{itemize}
\end{proof}
\newpage

\subsubsection{函数空间的完备性和Blackwell定理}
\textbf{当然, 如果我们要把Banach不动点定理用于解Bellman方程, 首先需要说明定义Bellman算子的函数空间是完备的度量空间(当然按书中用sup-norm来诱导度量的话, 就可以得到一个Banach空间); 其次还要说明Bellman算子是一个压缩映射.本节主要介绍解决这两个问题的两个定理.}
\begin{theorem}{completeness of continuous-function space}
    Let $X\subset \mathbb{R}^L$, $C(X)$ be the set of bounded continuous function $f:X\to\mathbb{R}$ with the sup-norm.
    Then $(C(X),\|\cdot\|)$ is a Banach Space.
\end{theorem}
\begin{proof}
    Let $\{f_n\}_{n=0}^{\infty}$ be a Cauchy Sequence i.e. for all $\varepsilon>0$, exists $N_\varepsilon\in\mathbb{N}$ such that 
    $\|f_n-f_m\|=\sup_{x\in X}|f_n(x)-f_m(x)|\leq \varepsilon,\forall n,m\geq N_\varepsilon$.
    We want to show that there exists a function $f\in C(X)$ such that $\|f_n-f\|\to 0$.
    \begin{itemize}
        \item Find the $f$.\\
        Fixed $\bar{x}\in X$, we got a sequence $\{f_n(\bar{x})\}_{n=0}^{\infty}$.\\
        Since $\{f_n\}_{n=0}^{\infty}$ is a Cauchy Sequence, for any $\varepsilon>0$, there exists $N_\varepsilon$ such that
        $$|f_n(\bar{x})-f_m(\bar{x})|\leq \sup_{x\in X}|f_n(x)-f_m(x)|\leq \varepsilon,\forall n,m\geq N_\varepsilon$$
        so the $\{f_n (\bar{x})\}_{n=0}^{\infty}$ is a Cauchy Sequence in $\mathbb{R}$, it converges to a limit $f(\bar{x})\in\mathbb{R}$.\\
        Now, we define a function $f:X\to\mathbb{R}$, $f(x)=\lim_{n\to\infty}f_n(x),\forall x\in X$.
        
        \item Show that $\sup_{x\in X}|f_n(x)-f(x)|\to 0$.\\
        We want to show that $\lim_{n\to\infty}\sup_{x\in X}|f_n(x)-f(x)|=0$ i.e. $\forall \varepsilon>0$, $\exists N_\varepsilon\in\mathbb{N}$ s.t. 
        $\sup_{x\in X}|f_n(x)-f(x)|\leq \varepsilon,\forall n\geq N_\varepsilon$.
        $$\sup_{x\in X}|f_n(x)-f(x)|=\sup_{x\in X}|(f_n(x)-f_m(x))+(f_m(x)-f(x))|\leq \sup_{x\in X}|f_n(x)-f_m(x)|+\sup_{x\in X}|f_m(x)-f(x)|$$
        Since $\{f_n\}_{n=0}^{\infty}$ is a Cauchy Sequence, $\sup_{x\in X}|f_n(x)-f_m(x)|\to 0$;\\
        Since $\{f_m(x)\}_{m=0}^{\infty}$ is a convergent sequence on $\mathbb{R}$ for any $x\in X$,with the definition of $f$, for any $m\to\infty$, $\sup_{x\in X}|f_m(x)-f(x)|\to 0$.\\
        Then $\sup_{x\in X}|f_n(x)-f(x)|\to 0$.

        \item Proof that $f\in C(X)$.\\
        We need to proof that $f\in C(X)$ i.e. $f$ is a bounded continuous function.\\
        Obviously, $f$ is bounded, Otherwise, $f_n$ could not be bounded due to  the covegence.\\
        Besides, $|f(x)-f(x_0)|=|(f(x)-f_n(x))+(f_n(x)-f_n(x_0))+(f_n(x_0)-f(x_0))|\leq |(f(x)-f_n(x))|+|(f_n(x)-f_n(x_0))|+|(f_n(x_0)-f(x_0))|$\\
        Since $f_n$ is continous, $|f_n(x)-f_n(x_0)|\to 0$; Since $\{f_n(x_0)\}$ converge to $f(x_0)$ (we have already proved it), $|f_n(x_0)-f(x_0)|\to 0$, similarly, $|f(x)-f_n(x)|\to 0$.

    \end{itemize}
\end{proof}
\noindent 注:\\
1. 证明完备性需要我们找到Cauchy列的极限, 我们的思路是先固定$x$, 通过实数的完备性得到点态收敛, 再证明在范数下函数收敛(事实上依此处定义的范数收敛等价于一致收敛).\\
2. 从逻辑上来说, 到第二部分为止的证明是不够的, 因为我们没有证明$f\in C(X)$, 也就是说, $f$虽然满足了$\sup |f_n(x)-f(x)|$收敛到零, 但它还不是这个范数所作用的对象, 这一细节还需要进一步地证明.\\
3. 更自然的想法可能会先证明$f\in C(X)$再证明$\|f_n-f\|\to 0 $, 但在这里有点反其道而行, 主要是为了得到更多的估计工具.\\
4. 有人可能会困惑为什么这里多了有界的条件, 这主要是因为这儿没说$X$是紧集, 如果声明$X$是紧集的话, 有界性自动成立.\\
5. 使用压缩映射定理只要在完备度量空间就够了, 但这里定义的范数性质比较好, 所以我们可以得到一个Banach空间.\\

在使用Banach不动点定理之前, 我们得先验证Bellman算子是一个压缩映射, 证明压缩映射一般会用到各种不等式, 但由于Bellman算子比较特别, 用不等式很难对其估计, 我们需要开辟新的方法.这里给出验证函数是压缩映射的一个定理——Blackwell定理.
\begin{theorem}{Blackwell Theorem}
    Let $X\subset\mathbb{R}^L$ and $C(X)$ be the set of bounded continuous function $f:X\to\mathbb{R}$ with the sup-norm.
    Let $T:C(X)\to C(X)$ be a operator satisfying:
    \begin{itemize}
        \item Monotoniacity: $f\geq g\Rightarrow Tf\geq Tg$ for all $f,g\in C(X)$;
        \item Discounting: Let the function $f+a$ for $f\in C(X)$ and $a\in\mathbb{R}^n_+$ defined by $(f+a)(x)=f(x)+a$.There exists a $\beta\in(0,1)$ such that for all $f\in C(X)$,$a\geq 0$ and all $x\in X$, $[T(f+a)(x)]\leq [Tf](x)+\beta a$.
    \end{itemize}
   If these two conditions are satisfied, then $T$ is a contraction mapping with module $\beta$.
\end{theorem}

注:
\begin{itemize}
    \item 虽然Blakcwell定理很好用, 但在实际运用中, 它能发挥的作用很有限(比如很常用的CD型生产函数就可能导致不满足有界性)
    \item 通过Blakcwell定理, 可以证明Bellman算子具有很好的压缩映射的性质(如果假设足够强的话, 这也是经济学很操蛋的地方, 我们总是用一组很强的假设推出一个看上去很美观很强的结论, 但在现实问题中, 这一组很强的假设是远远不够用的).
\end{itemize}
\section{集值映射与最大值原理}
这一部分主要说明序列问题和递归问题的等价性.
\section{马尔可夫过程}
这里讨论引入风险(随机性)的情形.